Epochs = 10
Batch_size = 64

inputs = keras.Input(shape=(28,28,1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation="softmax")(x)

model = keras.Model(inputs=inputs, outputs=outputs)

model.summary()

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000,28,28,1))
train_images = train_images.astype("float32") / 255

test_images = test_images.reshape((10000,28,28,1))
test_images = test_images.astype("float32") / 255

model.compile(optimizer="rmsprop",loss="sparse_categorical_crossentropy", metrics=['accuracy'])

model.fit(train_images,train_labels, epochs=Epochs, batch_size=Batch_size)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"test acc : {test_acc:.3f}")


---------------------------------------------------


import matplotlib.pyplot as plt

import os
from PIL import Image

# def check_images(directory):
#     for root, _, files in os.walk(directory):
#         for file in files:
#             file_path = os.path.join(root, file)
#             try:
#                 with Image.open(file_path) as img:
#                     img.verify()  # 이미지 파일 검증
#             except (IOError, SyntaxError) as e:
#                 print(f"Removing corrupted file: {file_path}")
#                 os.remove(file_path)
#
# check_images("data/archive/PetImages")

original_dir = pathlib.Path("data/archive/PetImages")
new_base_dir = pathlib.Path("cats_vs_dogs_small")

train_dataset = image_dataset_from_directory(new_base_dir/"train",
                                             image_size=(180,180),
                                             batch_size=32)
vaildation_dataset = image_dataset_from_directory(new_base_dir/"validation",
                                             image_size=(180,180),
                                             batch_size=32)

test_dataset = image_dataset_from_directory(new_base_dir/"test",
                                             image_size=(180,180),
                                             batch_size=32)

Epochs = 1
Batch_size = 64

inputs = keras.Input(shape=(180,180,3))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=256, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)

model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer="rmsprop",loss="binary_crossentropy", metrics=['accuracy'])

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="convnet_from_scratch.keras",
        save_best_only=True,
        monitor="val_loss")
]

history =  model.fit(train_dataset,
                     epochs=Epochs,
                     validation_data=vaildation_dataset,
                     callbacks=callbacks)

test_model = keras.models.load_model("convnet_from_scratch.keras")

test_loss, test_acc = model.evaluate(test_model)
print(f"test acc : {test_acc:.3f}")


# PC 환경
# GPU: RTX 4070 SUPER
# CUDA: v.11.8
# CUDNN: v.8.6.0

# Anaconda 환경
# Python: v 3.8.20
# TensorFlow-gpu: v.2.12.0

import os  # 파일 및 디렉토리 관리를 위한 라이브러리
import shutil  # 파일 이동 및 복사를 위한 라이브러리
import tensorflow as tf  # 딥러닝 모델링을 위한 TensorFlow
import numpy as np

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory

from google.colab import drive  # Google Drive를 마운트하기 위한 라이브러리

os.environ["CUDA_VISIBLE_DEVICES"] = "0"    # 텐서플로가 GPU를 사용하도록 설정

# 구글 드라이브 마운트
drive.mount('/content/drive')  # Colab에서 Google Drive에 접근할 수 있도록 마운트

# 데이터 경로 설정
base_dir = '/content/drive/My Drive/RPS Data'  # Google Drive에서 RPS Data 폴더 경로
train_dir = os.path.join(base_dir, 'train')  # 학습 데이터(train) 폴더 경로
test_dir = os.path.join(base_dir, 'test')  # 테스트 데이터(test) 폴더 경로
-------------------


def make_subset(subset_name, start_index, end_index):
    for category in ("Cat", "Dog"):  # 'Cat'과 'Dog'로 원본 폴더 이름을 그대로 사용
        dir = new_base_dir / subset_name / category
        os.makedirs(dir, exist_ok=True)  # 디렉토리 생성
        file_names = sorted((original_dir / category).iterdir(), key=lambda x: x.name)  # 정렬된 파일 목록

        # 지정된 범위의 파일 복사
        for i, src_path in enumerate(file_names[start_index:end_index]):
            dst_path = dir / f"{category.lower()}.{i + start_index}.jpg"  # cat.0.jpg 등으로 저장
            shutil.copyfile(src=src_path, dst=dst_path)


make_subset("train", start_index=0, end_index=1000)
make_subset("validation", start_index=1000, end_index=1500)
make_subset("test", start_index=1500, end_index=2500)
